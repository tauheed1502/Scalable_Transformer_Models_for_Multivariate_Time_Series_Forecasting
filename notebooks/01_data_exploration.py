#!/usr/bin/env python3
"""
Large-Scale Time Series Forecasting - Data Exploration
Executable Python version of 01_data_exploration.ipynb

Key Objectives:
- Analyze data quality and coverage across 50+ financial and macroeconomic series
- Identify non-stationarity patterns requiring advanced feature engineering
- Assess cross-series correlations and regime characteristics
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
from fredapi import Fred
import warnings
warnings.filterwarnings('ignore')

# Import project modules
import sys
import os
sys.path.append('../')
from config import config
from src.data_preprocessing import DataCollector

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette('husl')

def main():
    """Main data exploration pipeline"""
    
    print(f'üîç EXPLORING TIME SERIES DATA')
    print(f'üìä Target: {len(config.FINANCIAL_TICKERS)} financial + {len(config.MACRO_INDICATORS)} macro series')
    print(f'üìÖ Period: {config.START_DATE} to {config.END_DATE}')
    print('=' * 80)
    
    # Initialize data collector
    collector = DataCollector()
    
    # ========== 1. DATA COLLECTION ==========
    print('\nüì• SECTION 1: DATA COLLECTION')
    print('-' * 50)
    
    # Collect financial data
    print('Collecting financial data...')
    financial_data = collector.collect_financial_data()
    print(f'‚úÖ Successfully collected {len(financial_data)} financial series')
    print(f'‚ùå Failed to collect: {len(config.FINANCIAL_TICKERS) - len(financial_data)} series')
    
    # Display data summary
    data_summary = []
    for ticker, data in financial_data.items():
        data_summary.append({
            'Ticker': ticker,
            'Records': len(data),
            'Start_Date': data.index.min().strftime('%Y-%m-%d'),
            'End_Date': data.index.max().strftime('%Y-%m-%d'),
            'Missing_Values': data.isnull().sum().sum(),
            'Avg_Price': data['Close'].mean().round(2),
            'Volatility': (data['Close'].pct_change().std() * 252**0.5 * 100).round(2)
        })
    
    summary_df = pd.DataFrame(data_summary)
    print('\nFinancial Data Summary:')
    print(summary_df.head(10).to_string(index=False))
    
    # Collect macro data
    print('\nCollecting macroeconomic data...')
    macro_data = collector.collect_macro_data()
    print(f'‚úÖ Successfully collected {len(macro_data)} macro series')
    
    if macro_data:
        macro_summary = []
        for indicator, data in macro_data.items():
            macro_summary.append({
                'Indicator': indicator,
                'Records': len(data.dropna()),
                'Start_Date': data.dropna().index.min().strftime('%Y-%m-%d'),
                'End_Date': data.dropna().index.max().strftime('%Y-%m-%d'),
                'Missing_Pct': (data.isnull().sum() / len(data) * 100).round(2),
                'Mean_Value': data.mean().round(2),
                'Std_Dev': data.std().round(2)
            })
        
        macro_summary_df = pd.DataFrame(macro_summary)
        print('\nMacro Data Summary:')
        print(macro_summary_df.to_string(index=False))
    
    # ========== 2. TIME SERIES VISUALIZATION ==========
    print('\nüìà SECTION 2: TIME SERIES VISUALIZATION')
    print('-' * 50)
    
    # Plot sample financial time series
    sample_tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']
    available_tickers = [t for t in sample_tickers if t in financial_data]
    
    if available_tickers:
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        for i, ticker in enumerate(available_tickers[:4]):
            row, col = i // 2, i % 2
            data = financial_data[ticker]
            axes[row, col].plot(data.index, data['Close'], label=f'{ticker} Close Price')
            axes[row, col].set_title(f'{ticker} Stock Price Evolution')
            axes[row, col].set_ylabel('Price ($)')
            axes[row, col].legend()
            axes[row, col].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/plots/sample_financial_series.png', dpi=300, bbox_inches='tight')
        plt.show()
        print('‚úÖ Sample financial series visualization saved')
    
    # ========== 3. STATIONARITY ANALYSIS ==========
    print('\nüî¨ SECTION 3: STATIONARITY ANALYSIS')
    print('-' * 50)
    
    from statsmodels.tsa.stattools import adfuller
    
    stationarity_results = []
    test_series = list(financial_data.keys())[:8]  # Test subset
    
    for ticker in test_series:
        data = financial_data[ticker]
        
        # Test price levels
        try:
            price_adf = adfuller(data['Close'].dropna())
            price_stationary = price_adf[1] < 0.05
        except:
            price_adf = [0, 1, 0, 0, {}]
            price_stationary = False
        
        # Test returns
        try:
            returns_adf = adfuller(data['Returns'].dropna())
            returns_stationary = returns_adf[1] < 0.05
        except:
            returns_adf = [0, 1, 0, 0, {}]
            returns_stationary = False
        
        stationarity_results.append({
            'Ticker': ticker,
            'Price_ADF_Stat': round(price_adf[0], 3),
            'Price_p_value': round(price_adf[1], 3),
            'Price_Stationary': 'Yes' if price_stationary else 'No',
            'Returns_ADF_Stat': round(returns_adf[0], 3),
            'Returns_p_value': round(returns_adf[1], 3),
            'Returns_Stationary': 'Yes' if returns_stationary else 'No'
        })
    
    stationarity_df = pd.DataFrame(stationarity_results)
    print('Stationarity Test Results (ADF Test):')
    print(stationarity_df.to_string(index=False))
    
    # ========== 4. CORRELATION ANALYSIS ==========
    print('\nüîó SECTION 4: CROSS-SERIES CORRELATION')
    print('-' * 50)
    
    # Create correlation matrix for major stocks
    major_stocks = [t for t in ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'META', 'NVDA', 'JPM'] 
                   if t in financial_data]
    
    if len(major_stocks) >= 4:
        returns_data = {}
        for ticker in major_stocks:
            returns_data[ticker] = financial_data[ticker]['Returns']
        
        returns_df = pd.DataFrame(returns_data)
        correlation_matrix = returns_df.corr()
        
        # Plot correlation heatmap
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                    square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
        plt.title('Cross-Series Return Correlations\n(Foundation for Lagged Correlation Features)')
        plt.tight_layout()
        plt.savefig('../results/plots/correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
        print(f'‚úÖ Average cross-correlation: {avg_correlation:.3f}')
    
    # ========== 5. VOLATILITY ANALYSIS ==========
    print('\nüìä SECTION 5: VOLATILITY CLUSTERING')
    print('-' * 50)
    
    # Analyze volatility clustering for sample ticker
    sample_ticker = available_tickers[0] if available_tickers else list(financial_data.keys())[0]
    sample_data = financial_data[sample_ticker]
    sample_returns = sample_data['Returns'].dropna()
    
    if len(sample_returns) > 100:
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        
        # Returns over time
        axes[0].plot(sample_returns.index, sample_returns, alpha=0.7, linewidth=0.5)
        axes[0].set_title(f'{sample_ticker} Daily Returns - Volatility Clustering Evidence')
        axes[0].set_ylabel('Daily Returns')
        axes[0].grid(True, alpha=0.3)
        
        # Rolling volatility
        rolling_vol = sample_returns.rolling(window=20).std() * np.sqrt(252) * 100
        axes[1].plot(rolling_vol.index, rolling_vol, color='red', linewidth=1)
        axes[1].set_title(f'{sample_ticker} Rolling 20-Day Volatility (Annualized %)')
        axes[1].set_ylabel('Volatility (%)')
        axes[1].set_xlabel('Date')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('../results/plots/volatility_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        print(f'‚úÖ Volatility analysis completed for {sample_ticker}')
    
    # ========== 6. DATA QUALITY ASSESSMENT ==========
    print('\n‚úÖ SECTION 6: DATA QUALITY ASSESSMENT')
    print('-' * 50)
    
    quality_metrics = []
    
    for ticker, data in financial_data.items():
        quality_metrics.append({
            'Series': f'financial_{ticker}',
            'Length': len(data),
            'Missing_Pct': (data.isnull().sum().sum() / (len(data) * len(data.columns)) * 100).round(2),
            'Zero_Returns_Pct': ((data['Returns'] == 0).sum() / len(data) * 100).round(2) if 'Returns' in data.columns else 0,
            'Extreme_Returns_Pct': ((np.abs(data['Returns']) > data['Returns'].std() * 3).sum() / len(data) * 100).round(2) if 'Returns' in data.columns else 0,
            'Data_Coverage': f"{((data.index.max() - data.index.min()).days / 365.25 * 100):.1f}%"
        })
    
    quality_df = pd.DataFrame(quality_metrics)
    
    print('Data Quality Assessment:')
    print(f'üìä Total Series: {len(quality_df)}')
    print(f'üìè Average Length: {quality_df["Length"].mean():.0f} records')
    print(f'‚ùì Average Missing Data: {quality_df["Missing_Pct"].mean():.2f}%')
    print(f'‚úÖ High Coverage Series: {len(quality_df[quality_df["Missing_Pct"] < 5])}')
    
    # ========== 7. SUMMARY AND NEXT STEPS ==========
    print('\n' + '=' * 80)
    print('üìã DATA EXPLORATION SUMMARY')
    print('=' * 80)
    
    print(f'üìä Data Collected:')
    print(f'   ‚Ä¢ {len(financial_data)} financial time series')
    print(f'   ‚Ä¢ {len(macro_data)} macroeconomic indicators')
    print(f'   ‚Ä¢ Average series length: {quality_df["Length"].mean():.0f} records')
    
    print(f'\nüîç Key Findings:')
    print('   ‚úì Price levels show non-stationarity (requiring differencing)')
    print('   ‚úì Returns exhibit volatility clustering (GARCH features needed)')
    print('   ‚úì Strong cross-series correlations detected')
    print('   ‚úì High data quality with minimal missing values')
    
    print(f'\n‚öôÔ∏è  Next Steps:')
    print('   1. Advanced feature engineering (volatility, regime shifts, Fourier)')
    print('   2. Data preprocessing for transformer models')
    print('   3. Model training with TFT and Informer architectures')
    print('   4. Performance evaluation vs. baseline methods')
    
    # Save summary data
    summary_df.to_csv('../results/financial_data_summary.csv', index=False)
    quality_df.to_csv('../results/data_quality_assessment.csv', index=False)
    
    if macro_data:
        macro_summary_df.to_csv('../results/macro_data_summary.csv', index=False)
    
    print(f'\nüíæ Summary files saved to ../results/')
    
    return {
        'financial_series': len(financial_data),
        'macro_series': len(macro_data),
        'total_series': len(financial_data) + len(macro_data),
        'avg_length': quality_df["Length"].mean(),
        'data_quality_score': 100 - quality_df["Missing_Pct"].mean()
    }

if __name__ == "__main__":
    results = main()
    print(f"\nüéØ Data exploration completed successfully!")
    print(f"üìà Ready for feature engineering pipeline")
